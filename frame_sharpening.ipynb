{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"frame_sharpening.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"u9NS6hZyZUlN","executionInfo":{"status":"ok","timestamp":1602200429013,"user_tz":300,"elapsed":4484,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"798ffae6-ab44-41a6-aaf6-600b4bdc9c00","colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["import numpy as np\n","from sklearn.feature_extraction import image\n","import imageio\n","from matplotlib.pyplot import imshow\n","import tensorflow as tf\n","tf.config.list_physical_devices('GPU')"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import random\n","from tqdm import tqdm\n","\n","# use this to split a dataset in one folder into training and validation folders\n","def splitAndMoveData(data_folder, train_folder=\"data/training/\", val_folder=\"data/validation/\", val_split=0.1):\n","    filenames = os.listdir(data_folder)\n","    random.shuffle(filenames)\n","\n","    train_files = filenames[:len(filenames) - int(len(filenames) * val_split)]\n","    val_files = filenames[len(filenames) - int(len(filenames) * val_split):]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    for f in tqdm(train_files):\n","        os.replace(data_folder + f, train_folder + f)\n","\n","    for f in tqdm(val_files):\n","        os.replace(data_folder + f, val_folder + f)\n","\n","# use this to get the training and validation file paths for the data generators\n","def getFilePaths(train_folder=\"data/training/\", val_folder=\"data/validation/\"):\n","    train_files = [train_folder + f for f in os.listdir(train_folder)]\n","    val_files = [val_folder + f for f in os.listdir(val_folder)]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    return train_files, val_files\n","\n"]},{"cell_type":"code","metadata":{"id":"XoA3xF-G8gZD","executionInfo":{"status":"ok","timestamp":1602200433825,"user_tz":300,"elapsed":359,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import os\n","import keras\n","\n","train_files, val_files = getFilePaths()\n","\n","patch_size = (100, 100)\n","y_stride = 25\n","x_stride = 25\n","batch_size = 1 # number of images to get patches from\n","patches_per_batch = 1 # total number of patches per batch of images\n","max_patches = patches_per_batch // batch_size   # number of patches to get from each image in the batch\n","train_steps = len(train_files) // batch_size\n","val_steps =  len(val_files) // batch_size"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["32850 training files and 3650 validation files\n"]}]},{"cell_type":"code","metadata":{"id":"8vD4bh1OeRXI","executionInfo":{"status":"ok","timestamp":1602200771553,"user_tz":300,"elapsed":422,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import cv2\n","import imageio\n","import numpy as np\n","from sklearn.feature_extraction import image\n","import keras\n","from PIL import Image\n","import random\n","\n","class data_generator(keras.utils.Sequence):\n","    def __init__(self):\n","        self.filenames = train_files if self.train else val_files\n","        self.batch_size = batch_size\n","        self.patch_size = patch_size\n","        self.patch_h = patch_size[1]\n","        self.patch_w = patch_size[0]\n","        self.max_patches = max_patches\n","        self.x_stride = x_stride\n","        self.y_stride = y_stride\n","        \n","\n","    def __len__(self) :\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n","    def __iter__(self):\n","        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n","        while 1:\n","            for item in (self[i] for i in range(len(self))):\n","                yield item\n","\n","    def load(self, path):\n","        #img = Image.open(path)\n","        #img = np.array(img, dtype=np.uint8)\n","        img = keras.preprocessing.image.load_img(path)\n","        img = keras.preprocessing.image.img_to_array(img)\n","\n","        return img / 255.\n","    \n","    def pixalate_image(self, image, scale_percent = 40):\n","      og_w = image.shape[1]\n","      og_h = image.shape[0]\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = og_w\n","      height = og_h\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","    \n","    def getPatchRanges(self, img):\n","      ranges = []\n","      for y in range(self.patch_h, img.shape[0] + 1 + (self.y_stride - (img.shape[0] % self.y_stride)), self.y_stride):\n","        if y > img.shape[0]:\n","          y = img.shape[0]\n","\n","        for x in range(self.patch_w, img.shape[1] + 1 + (self.x_stride - (img.shape[1] % self.x_stride)), self.x_stride):\n","          if x > img.shape[1]:\n","            x = img.shape[1]\n","\n","          ranges.append([y - self.patch_h, y, x - self.patch_w, x])\n","\n","      return ranges\n","\n","    def extractPatches(self, img, ranges):\n","      return [img[y0 : y1, x0 : x1] for y0, y1, x0, x1 in ranges]\n","\n","    def __getitem__(self, idx):\n","        batch_ids = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n","        og_images = [self.load(f) for f in batch_ids]#np.array(list(map(self.load, batch_ids)))\n","        pixelated_images = [self.pixalate_image(img, 25) for img in og_images]#np.array(list(map(self.pixalate_image, images)))\n","\n","        if not self.train:\n","          np.random.seed(0)\n","\n","        target_patches = []\n","        input_patches = []\n","        for i, img in enumerate(og_images):\n","          if not self.train:\n","            ranges = self.getPatchRanges(img)\n","            idxs = np.random.choice(list(range(len(ranges))), size = max_patches)\n","            ranges = [ranges[i] for i in idxs]\n","          else:\n","            ranges = random.sample(self.getPatchRanges(img), max_patches)\n","\n","          target_patches.extend(self.extractPatches(img, ranges))\n","          input_patches.extend(self.extractPatches(pixelated_images[i], ranges))\n","        \n","        target_patches = np.array(target_patches)\n","        input_patches = np.array(input_patches)\n","        return input_patches, target_patches\n","        \n","class train_generator(data_generator):\n","  def __init__(self):\n","    self.train = True\n","    super().__init__()\n","\n","class val_generator(data_generator):\n","  def __init__(self):\n","    self.train = False\n","    super().__init__()\n","\n","# loader functions for the generators needed by tensorflow\n","# in order to use interleave   \n","def get_train_dataset(self):\n","    train = True\n","    self = tf.data.Dataset.from_generator(\n","        train_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self\n","\n","def get_val_dataset(self):\n","    train = False\n","    self = tf.data.Dataset.from_generator(\n","        val_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxBoXwL1-29q","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200553040,"user_tz":300,"elapsed":420,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","from tensorflow.keras import Model, Input, regularizers\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def getModel():\n","  Input_img = Input(shape = patch_size + (3,))  \n","      \n","  #encoding architecture\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n","  x2 = MaxPool2D( (2, 2))(x2)\n","  encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n","\n","  # decoding architecture\n","  x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n","  x3 = UpSampling2D((2, 2))(x3)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n","  decoded = Conv2D(3, (3, 3), padding='same')(x1)\n","\n","  autoencoder = Model(Input_img, decoded)\n","  #autoencoder.summary()\n","  autoencoder.compile(optimizer='adam', loss='mse')\n","\n","  return autoencoder"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZK6yESWm_-Am","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200776490,"user_tz":300,"elapsed":508,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","# from each of the generators create a pair of interleaved datasets\n","# tensorflow can automatically multiprocess interleaved datasets\n","# so that while batches can be loaded and processed ahead of time\n","interleaved_train = tf.data.Dataset.range(2).interleave(\n","                    get_train_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )\n","interleaved_val = tf.data.Dataset.range(2).interleave(\n","                    get_val_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKLhe3NEAs8q","executionInfo":{"status":"error","timestamp":1602200780661,"user_tz":300,"elapsed":3226,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"3bb21567-5f60-448c-a722-e8c4de117097","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n","\n","autoencoder = getModel()\n","\n","a_e = autoencoder.fit(interleaved_train,\n","                   steps_per_epoch = train_steps,\n","                   epochs = 50,\n","                    verbose=1,\n","                    validation_data=interleaved_val,\n","                    validation_steps=val_steps,\n","                   callbacks=callbacks)\n","\n","autoencoder.save(\"/content/drive/My Drive/frame_interpolation/test_encoder3.h5\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"error","ename":"UnknownError","evalue":"2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-8-1778a8b5373a>:11) ]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-8-1778a8b5373a>:11) ]]\n\t [[mean_squared_error/cond/then/_0/mean_squared_error/cond/cond/then/_59/mean_squared_error/cond/cond/remove_squeezable_dimensions/Equal/_42]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1451]\n\nFunction call stack:\ntrain_function -> train_function\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-8-1778a8b5373a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterleaved_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                    callbacks=callbacks)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/drive/My Drive/frame_interpolation/test_encoder3.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32mc:\\VS_code_projects\\frameSharpening\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-8-1778a8b5373a>:11) ]]\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_1/conv2d/Relu (defined at <ipython-input-8-1778a8b5373a>:11) ]]\n\t [[mean_squared_error/cond/then/_0/mean_squared_error/cond/cond/then/_59/mean_squared_error/cond/cond/remove_squeezable_dimensions/Equal/_42]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1451]\n\nFunction call stack:\ntrain_function -> train_function\n"]}]},{"cell_type":"code","metadata":{"id":"oprjongPPL7w","executionInfo":{"status":"aborted","timestamp":1602198904197,"user_tz":300,"elapsed":4473,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["from tqdm import tqdm\n","\n","def reconstruct_image(patches, ranges, weights):\n","  img = np.zeros(weights.shape)\n","  for i, (y0, y1, x0, x1) in enumerate(ranges):\n","    img[y0:y1,x0:x1] += patches[i]\n","  \n","  return np.array((img / weights) * 255, dtype=np.uint8)\n","\n","def sharpen_image(model, img):\n","  patches = []\n","  weights = np.zeros(img.shape)\n","  ranges = []\n","  for y in range(patch_size[1], img.shape[0] + 1 + (y_stride - (img.shape[0] % y_stride)), y_stride):\n","    if y > img.shape[0]:\n","      y = img.shape[0]\n","\n","    for x in range(patch_size[0], img.shape[1] + 1 + (x_stride - (img.shape[1] % x_stride)), x_stride):\n","      if x > img.shape[1]:\n","        x = img.shape[1]\n","\n","      y0 = y - patch_size[1]\n","      x0 = x - patch_size[0]\n","      patches.append(img[y0 : y, x0 : x])\n","      weights[y0 : y, x0 : x] += 1\n","      ranges.append([y0, y, x0, x])\n","\n","  patches = np.array(patches)\n","  patch_batch_count = batch_size * max_patches\n","  preds = []\n","  for i in tqdm(list(range(0, patches.shape[0], patch_batch_count))):\n","    preds.append(np.array(model.predict(patches[i : i + patch_batch_count])))\n","  \n","  preds = np.concatenate(preds)\n","\n","  return reconstruct_image(preds, ranges, weights)\n","\n","def pixalate_image(image, scale_percent = 40):\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = image.shape[1]\n","      height = image.shape[0]\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"im0CW_slCn0D","executionInfo":{"status":"aborted","timestamp":1602198904198,"user_tz":300,"elapsed":4467,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["\"\"\"from tensorflow.keras import models \n","autoencoder = models.load_model(\"/content/drive/My Drive/frame_interpolation/test_encoder2.h5\")\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfK6npAvQ3LJ","executionInfo":{"status":"aborted","timestamp":1602198904198,"user_tz":300,"elapsed":4459,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import cv2\n","img = keras.preprocessing.image.load_img(\"/content/drive/My Drive/frame_interpolation/test3.png\")\n","img = keras.preprocessing.image.img_to_array(img) / 255\n","dim = (img.shape[1] * 2, img.shape[0] * 2)\n","print(img.shape)\n","img = pixalate_image(img, 25)#cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n","img = np.array(img)\n","imageio.imwrite(\"/content/drive/My Drive/frame_interpolation/test3Pixelated.png\", np.array(img * 255, dtype=np.uint8))\n","print(img.shape)\n","imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRXr9wBVR6mW","executionInfo":{"status":"aborted","timestamp":1602198904199,"user_tz":300,"elapsed":4451,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["sharp_img = sharpen_image(autoencoder, img)\n","imshow(sharp_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gtzzXjzq_a9","executionInfo":{"status":"aborted","timestamp":1602198904199,"user_tz":300,"elapsed":4443,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["imageio.imsave(\"/content/drive/My Drive/frame_interpolation/test3out.png\", sharp_img)"],"execution_count":null,"outputs":[]}]}