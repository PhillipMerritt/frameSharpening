{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"frame_sharpening.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"Python 3.6.6 64-bit ('env')","display_name":"Python 3.6.6 64-bit ('env')","metadata":{"interpreter":{"hash":"9a3576fbf32d4002ec41f297a6260da3a7f919f0ab590bc1e7359cb7872256e2"}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"u9NS6hZyZUlN","executionInfo":{"status":"ok","timestamp":1602200429013,"user_tz":300,"elapsed":4484,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"798ffae6-ab44-41a6-aaf6-600b4bdc9c00","colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["import numpy as np\n","from sklearn.feature_extraction import image\n","import imageio\n","from matplotlib.pyplot import imshow\n","import tensorflow as tf\n","tf.config.list_physical_devices('GPU')"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import random\n","from tqdm import tqdm\n","\n","# use this to split a dataset in one folder into training and validation folders\n","def splitAndMoveData(data_folder, train_folder=\"data/training/\", val_folder=\"data/validation/\", val_split=0.1):\n","    filenames = os.listdir(data_folder)\n","    random.shuffle(filenames)\n","\n","    train_files = filenames[:len(filenames) - int(len(filenames) * val_split)]\n","    val_files = filenames[len(filenames) - int(len(filenames) * val_split):]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    for f in tqdm(train_files):\n","        os.replace(data_folder + f, train_folder + f)\n","\n","    for f in tqdm(val_files):\n","        os.replace(data_folder + f, val_folder + f)\n","\n","# use this to get the training and validation file paths for the data generators\n","def getFilePaths(train_folder=\"data/training/\", val_folder=\"data/validation/\"):\n","    train_files = [train_folder + f for f in os.listdir(train_folder)]\n","    val_files = [val_folder + f for f in os.listdir(val_folder)]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    return train_files, val_files\n","\n"]},{"cell_type":"code","metadata":{"id":"8vD4bh1OeRXI","executionInfo":{"status":"ok","timestamp":1602200771553,"user_tz":300,"elapsed":422,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import cv2\n","import imageio\n","import numpy as np\n","from sklearn.feature_extraction import image\n","import keras\n","from PIL import Image\n","import random\n","\n","class data_generator(keras.utils.Sequence):\n","    def __init__(self):\n","        self.patch_size = patch_size\n","        self.patch_h = patch_size[1]\n","        self.patch_w = patch_size[0]\n","        self.x_stride = x_stride\n","        self.y_stride = y_stride\n","        \n","\n","    def __len__(self) :\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n","    def __iter__(self):\n","        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n","        while 1:\n","            for item in (self[i] for i in range(len(self))):\n","                yield item\n","\n","    def load(self, path):\n","        #img = Image.open(path)\n","        #img = np.array(img, dtype=np.uint8)\n","        img = keras.preprocessing.image.load_img(path)\n","        img = keras.preprocessing.image.img_to_array(img)\n","\n","        return img / 255.\n","    \n","    def pixalate_image(self, image, scale_percent = 40):\n","      og_w = image.shape[1]\n","      og_h = image.shape[0]\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = og_w\n","      height = og_h\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","    \n","    def getPatchRanges(self, img):\n","      ranges = []\n","      for y in range(self.patch_h, img.shape[0] + 1 + (self.y_stride - (img.shape[0] % self.y_stride)), self.y_stride):\n","        if y > img.shape[0]:\n","          y = img.shape[0]\n","\n","        for x in range(self.patch_w, img.shape[1] + 1 + (self.x_stride - (img.shape[1] % self.x_stride)), self.x_stride):\n","          if x > img.shape[1]:\n","            x = img.shape[1]\n","\n","          ranges.append([y - self.patch_h, y, x - self.patch_w, x])\n","\n","      return ranges\n","\n","    def extractPatches(self, img, ranges):\n","      return [img[y0 : y1, x0 : x1] for y0, y1, x0, x1 in ranges]\n","\n","    def __getitem__(self, idx):\n","        batch_ids = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n","        og_images = [self.load(f) for f in batch_ids]#np.array(list(map(self.load, batch_ids)))\n","        pixelated_images = [self.pixalate_image(img, 25) for img in og_images]#np.array(list(map(self.pixalate_image, images)))\n","\n","        if not self.train:\n","          np.random.seed(0)\n","\n","        target_patches = []\n","        input_patches = []\n","        for i, img in enumerate(og_images):\n","          if not self.train:\n","            ranges = self.getPatchRanges(img)\n","            idxs = np.random.choice(list(range(len(ranges))), size = max_patches)\n","            ranges = [ranges[i] for i in idxs]\n","          else:\n","            ranges = random.sample(self.getPatchRanges(img), max_patches)\n","\n","          target_patches.extend(self.extractPatches(img, ranges))\n","          input_patches.extend(self.extractPatches(pixelated_images[i], ranges))\n","        \n","        target_patches = np.array(target_patches)\n","        input_patches = np.array(input_patches)\n","        return input_patches, target_patches\n","        \n","class train_generator(data_generator):\n","  def __init__(self):\n","    self.train = True\n","    self.filenames = train_files\n","    self.batch_size = batch_size\n","    self.max_patches = max_patches\n","    super().__init__()\n","\n","class val_generator(data_generator):\n","  def __init__(self):\n","    self.train = False\n","    self.filenames = val_files\n","    self.batch_size = val_batch_size\n","    self.max_patches = val_max_batches\n","    super().__init__()\n","\n","# loader functions for the generators needed by tensorflow\n","# in order to use interleave   \n","def get_train_dataset(self):\n","    train = True\n","    self = tf.data.Dataset.from_generator(\n","        train_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self\n","\n","def get_val_dataset(self):\n","    train = False\n","    self = tf.data.Dataset.from_generator(\n","        val_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxBoXwL1-29q","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200553040,"user_tz":300,"elapsed":420,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","from tensorflow.keras import Model, Input, regularizers\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def getModel():\n","  Input_img = Input(shape = patch_size + (3,))  \n","      \n","  #encoding architecture\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n","  x2 = MaxPool2D( (2, 2))(x2)\n","  encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n","\n","  # decoding architecture\n","  x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n","  x3 = UpSampling2D((2, 2))(x3)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n","  decoded = Conv2D(3, (3, 3), padding='same')(x1)\n","\n","  autoencoder = Model(Input_img, decoded)\n","  #autoencoder.summary()\n","  autoencoder.compile(optimizer='adam', loss='mse')\n","\n","  return autoencoder"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoA3xF-G8gZD","executionInfo":{"status":"ok","timestamp":1602200433825,"user_tz":300,"elapsed":359,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import os\n","import keras\n","\n","train_files, val_files = getFilePaths()\n","\n","patch_size = (100, 100)\n","y_stride = 25\n","x_stride = 25\n","batch_size = 16 # number of images to get patches from\n","patches_per_batch = 32 # total number of patches per batch of images\n","max_patches = patches_per_batch // batch_size   # number of patches to get from each image in the batch\n","train_steps = len(train_files) // batch_size\n","\n","# seperate batch size for validation so that you can include more patches per image\n","val_batch_size = 4\n","val_max_batches = patches_per_batch // val_batch_size\n","val_steps =  len(val_files) // val_batch_size"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["32850 training files and 3650 validation files\n"]}]},{"cell_type":"code","metadata":{"id":"ZK6yESWm_-Am","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200776490,"user_tz":300,"elapsed":508,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","# from each of the generators create a pair of interleaved datasets\n","# tensorflow can automatically multiprocess interleaved datasets\n","# so that while batches can be loaded and processed ahead of time\n","interleaved_train = tf.data.Dataset.range(2).interleave(\n","                    get_train_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )\n","interleaved_val = tf.data.Dataset.range(2).interleave(\n","                    get_val_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKLhe3NEAs8q","executionInfo":{"status":"error","timestamp":1602200780661,"user_tz":300,"elapsed":3226,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"3bb21567-5f60-448c-a722-e8c4de117097","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","patience=3,\n","mode='min', \n","restore_best_weights=True),\n","tf.keras.callbacks.ModelCheckpoint(\"data/model_checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n","monitor='val_loss', \n","save_best_only=True,\n","mode='min',\n","save_weights_only=True)\n","]\n","\n","autoencoder = getModel()\n","\n","# set this to none if starting from scratch\n","model_weights = None\n","\n","if model_weights:\n","    autoencoder.load_weights(model_weights)\n","\n","a_e = autoencoder.fit(interleaved_train,\n","                   steps_per_epoch = train_steps,\n","                   epochs = 10,\n","                    verbose=1,\n","                    validation_data=interleaved_val,\n","                    validation_batch_size=val_batch_size,\n","                    validation_steps=val_steps,\n","                   callbacks=callbacks)\n","\n","autoencoder.save(\"data/saved_models/encoder1.h5\")"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","   2/2053 [..............................] - ETA: 8:26 - loss: 0.1696WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1606s vs `on_train_batch_end` time: 0.3318s). Check your callbacks.\n","2053/2053 [==============================] - 1074s 523ms/step - loss: 0.0043 - val_loss: 0.0032\n","Epoch 2/10\n","2053/2053 [==============================] - 1068s 520ms/step - loss: 0.0047 - val_loss: 0.0044\n","Epoch 3/10\n","2053/2053 [==============================] - 1071s 522ms/step - loss: 0.0034 - val_loss: 0.0032\n","Epoch 4/10\n","2053/2053 [==============================] - 1033s 503ms/step - loss: 0.0033 - val_loss: 0.0033\n"]}]},{"cell_type":"code","metadata":{"id":"oprjongPPL7w","executionInfo":{"status":"aborted","timestamp":1602198904197,"user_tz":300,"elapsed":4473,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["from tqdm import tqdm\n","\n","def reconstruct_image(patches, ranges, weights):\n","  img = np.zeros(weights.shape)\n","  for i, (y0, y1, x0, x1) in enumerate(ranges):\n","    img[y0:y1,x0:x1] += patches[i]\n","  \n","  return np.array(np.clip((img / weights) * 255, 0.0, 255.0), dtype=np.uint8)\n","\n","def sharpen_image(model, img):\n","  patches = []\n","  weights = np.zeros(img.shape)\n","  ranges = []\n","  for y in range(patch_size[1], img.shape[0] + 1 + (y_stride - (img.shape[0] % y_stride)), y_stride):\n","    if y > img.shape[0]:\n","      y = img.shape[0]\n","\n","    for x in range(patch_size[0], img.shape[1] + 1 + (x_stride - (img.shape[1] % x_stride)), x_stride):\n","      if x > img.shape[1]:\n","        x = img.shape[1]\n","\n","      y0 = y - patch_size[1]\n","      x0 = x - patch_size[0]\n","      patches.append(img[y0 : y, x0 : x])\n","      weights[y0 : y, x0 : x] += 1\n","      ranges.append([y0, y, x0, x])\n","\n","  patches = np.array(patches)\n","  patch_batch_count = batch_size * max_patches\n","  preds = []\n","  for i in tqdm(list(range(0, patches.shape[0], patch_batch_count))):\n","    preds.append(np.array(model.predict(patches[i : i + patch_batch_count])))\n","  \n","  preds = np.concatenate(preds)\n","\n","  return reconstruct_image(preds, ranges, weights)\n","\n","def pixalate_image(image, scale_percent = 40):\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = image.shape[1]\n","      height = image.shape[0]\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","  \n"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfK6npAvQ3LJ","executionInfo":{"status":"aborted","timestamp":1602198904198,"user_tz":300,"elapsed":4459,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["from timeit import default_timer as timer\n","filenames = os.listdir('data/tests/')\n","\n","start = timer()\n","\n","# put some images in the data/tests/ path and this will pixelate the model will upscale them\n","for i, f in enumerate(filenames):\n","    img = keras.preprocessing.image.load_img(\"data/tests/\" + f)\n","    img = keras.preprocessing.image.img_to_array(img) / 255.\n","    img = pixalate_image(img, 25)\n","    img = np.array(img)\n","    #imageio.imwrite(\"data/test_outputs/{}lr.png\".format(i + 1), np.array(img * 255., dtype=np.uint8))\n","    sharp_img = sharpen_image(autoencoder, img)\n","    imageio.imsave(\"data/test_outputs/{}.png\".format(i + 1), np.hstack([np.array(img * 255., dtype=np.uint8), sharp_img]))\n","\n","print((timer() - start) / len(filenames))"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:02<00:00,  5.41it/s]\n","100%|██████████| 16/16 [00:02<00:00,  5.54it/s]\n","100%|██████████| 16/16 [00:02<00:00,  5.44it/s]\n","100%|██████████| 16/16 [00:02<00:00,  5.55it/s]\n","100%|██████████| 21/21 [00:03<00:00,  5.70it/s]\n","100%|██████████| 16/16 [00:02<00:00,  5.55it/s]\n","100%|██████████| 15/15 [00:02<00:00,  5.80it/s]\n","100%|██████████| 15/15 [00:02<00:00,  5.81it/s]\n","100%|██████████| 15/15 [00:02<00:00,  5.84it/s]\n","100%|██████████| 15/15 [00:02<00:00,  5.77it/s]\n","100%|██████████| 15/15 [00:02<00:00,  5.81it/s]\n","3.793466590909091\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}