{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"frame_sharpening.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"u9NS6hZyZUlN","executionInfo":{"status":"ok","timestamp":1602200429013,"user_tz":300,"elapsed":4484,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"798ffae6-ab44-41a6-aaf6-600b4bdc9c00","colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["import numpy as np\n","from sklearn.feature_extraction import image\n","import imageio\n","from matplotlib.pyplot import imshow\n","import tensorflow as tf\n","tf.config.list_physical_devices('GPU')"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import os\n","import random\n","from tqdm import tqdm\n","\n","# use this to split a dataset in one folder into training and validation folders\n","def splitAndMoveData(data_folder, train_folder=\"data/training/\", val_folder=\"data/validation/\", val_split=0.1):\n","    filenames = os.listdir(data_folder)\n","    random.shuffle(filenames)\n","\n","    train_files = filenames[:len(filenames) - int(len(filenames) * val_split)]\n","    val_files = filenames[len(filenames) - int(len(filenames) * val_split):]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    for f in tqdm(train_files):\n","        os.replace(data_folder + f, train_folder + f)\n","\n","    for f in tqdm(val_files):\n","        os.replace(data_folder + f, val_folder + f)\n","\n","# use this to get the training and validation file paths for the data generators\n","def getFilePaths(train_folder=\"data/training/\", val_folder=\"data/validation/\"):\n","    train_files = [train_folder + f for f in os.listdir(train_folder)]\n","    val_files = [val_folder + f for f in os.listdir(val_folder)]\n","\n","    print(\"{} training files and {} validation files\".format(len(train_files), len(val_files)))\n","\n","    return train_files, val_files\n","\n"]},{"cell_type":"code","metadata":{"id":"XoA3xF-G8gZD","executionInfo":{"status":"ok","timestamp":1602200433825,"user_tz":300,"elapsed":359,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import os\n","import keras\n","\n","train_files, val_files = getFilePaths()\n","\n","patch_size = (100, 100)\n","y_stride = 25\n","x_stride = 25\n","batch_size = 32 # number of images to get patches from\n","patches_per_batch = 64 # total number of patches per batch of images\n","max_patches = patches_per_batch // batch_size   # number of patches to get from each image in the batch\n","train_steps = len(train_files) // batch_size\n","val_steps =  len(val_files) // batch_size"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["32850 training files and 3650 validation files\n"]}]},{"cell_type":"code","metadata":{"id":"8vD4bh1OeRXI","executionInfo":{"status":"ok","timestamp":1602200771553,"user_tz":300,"elapsed":422,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import cv2\n","import imageio\n","import numpy as np\n","from sklearn.feature_extraction import image\n","import keras\n","from PIL import Image\n","import random\n","\n","class data_generator(keras.utils.Sequence):\n","    def __init__(self):\n","        self.filenames = train_files if self.train else val_files\n","        self.batch_size = batch_size\n","        self.patch_size = patch_size\n","        self.patch_h = patch_size[1]\n","        self.patch_w = patch_size[0]\n","        self.max_patches = max_patches\n","        self.x_stride = x_stride\n","        self.y_stride = y_stride\n","        \n","\n","    def __len__(self) :\n","        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n","    def __iter__(self):\n","        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n","        while 1:\n","            for item in (self[i] for i in range(len(self))):\n","                yield item\n","\n","    def load(self, path):\n","        #img = Image.open(path)\n","        #img = np.array(img, dtype=np.uint8)\n","        img = keras.preprocessing.image.load_img(path)\n","        img = keras.preprocessing.image.img_to_array(img)\n","\n","        return img / 255.\n","    \n","    def pixalate_image(self, image, scale_percent = 40):\n","      og_w = image.shape[1]\n","      og_h = image.shape[0]\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = og_w\n","      height = og_h\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","    \n","    def getPatchRanges(self, img):\n","      ranges = []\n","      for y in range(self.patch_h, img.shape[0] + 1 + (self.y_stride - (img.shape[0] % self.y_stride)), self.y_stride):\n","        if y > img.shape[0]:\n","          y = img.shape[0]\n","\n","        for x in range(self.patch_w, img.shape[1] + 1 + (self.x_stride - (img.shape[1] % self.x_stride)), self.x_stride):\n","          if x > img.shape[1]:\n","            x = img.shape[1]\n","\n","          ranges.append([y - self.patch_h, y, x - self.patch_w, x])\n","\n","      return ranges\n","\n","    def extractPatches(self, img, ranges):\n","      return [img[y0 : y1, x0 : x1] for y0, y1, x0, x1 in ranges]\n","\n","    def __getitem__(self, idx):\n","        batch_ids = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n","        og_images = [self.load(f) for f in batch_ids]#np.array(list(map(self.load, batch_ids)))\n","        pixelated_images = [self.pixalate_image(img, 50) for img in og_images]#np.array(list(map(self.pixalate_image, images)))\n","\n","        if not self.train:\n","          np.random.seed(0)\n","\n","        target_patches = []\n","        input_patches = []\n","        for i, img in enumerate(og_images):\n","          if not self.train:\n","            ranges = self.getPatchRanges(img)\n","            idxs = np.random.choice(list(range(len(ranges))), size = max_patches)\n","            ranges = [ranges[i] for i in idxs]\n","          else:\n","            ranges = random.sample(self.getPatchRanges(img), max_patches)\n","\n","          target_patches.extend(self.extractPatches(img, ranges))\n","          input_patches.extend(self.extractPatches(pixelated_images[i], ranges))\n","        \n","        target_patches = np.array(target_patches)\n","        input_patches = np.array(input_patches)\n","        return input_patches, target_patches\n","        \n","class train_generator(data_generator):\n","  def __init__(self):\n","    self.train = True\n","    super().__init__()\n","\n","class val_generator(data_generator):\n","  def __init__(self):\n","    self.train = False\n","    super().__init__()\n","\n","# loader functions for the generators needed by tensorflow\n","# in order to use interleave   \n","def get_train_dataset(self):\n","    train = True\n","    self = tf.data.Dataset.from_generator(\n","        train_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self\n","\n","def get_val_dataset(self):\n","    train = False\n","    self = tf.data.Dataset.from_generator(\n","        val_generator,\n","        output_types = (tf.float32, tf.float32))\n","    return self"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxBoXwL1-29q","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200553040,"user_tz":300,"elapsed":420,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","from tensorflow.keras import Model, Input, regularizers\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","def getModel():\n","  Input_img = Input(shape = patch_size + (3,))  \n","      \n","  #encoding architecture\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(Input_img)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n","  x2 = MaxPool2D( (2, 2))(x2)\n","  encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n","\n","  # decoding architecture\n","  x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n","  x3 = UpSampling2D((2, 2))(x3)\n","  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n","  x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n","  decoded = Conv2D(3, (3, 3), padding='same')(x1)\n","\n","  autoencoder = Model(Input_img, decoded)\n","  #autoencoder.summary()\n","  autoencoder.compile(optimizer='adam', loss='mse')\n","\n","  return autoencoder\n","def getNewModel():\n","  Input_img = Input(shape= patch_size + (3,))  \n","    \n","  #encoding architecture\n","  x1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(Input_img)\n","  x2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1)\n","  x3 = MaxPool2D(padding='same')(x2)\n","  x4 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x3)\n","  x5 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x4)\n","  x6 = MaxPool2D(padding='same')(x5)\n","  encoded = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x6)\n","  #encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n","  # decoding architecture\n","  x7 = UpSampling2D()(encoded)\n","  x8 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x7)\n","  x9 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x8)\n","  x10 = Add()([x5, x9])\n","  x11 = UpSampling2D()(x10)\n","  x12 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x11)\n","  x13 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x12)\n","  x14 = Add()([x2, x13])\n","  # x3 = UpSampling2D((2, 2))(x3)\n","  # x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)\n","  # x1 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n","  decoded = Conv2D(3, (3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l1(10e-10))(x14)\n","  autoencoder = Model(Input_img, decoded)\n","  autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","  autoencoder.summary()\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 100, 100, 64) 1792        input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 100, 100, 64) 36928       conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 50, 50, 128)  73856       max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 128)  0           conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 25, 25, 256)  295168      max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 50, 50, 256)  0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 50, 50, 128)  295040      up_sampling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 50, 50, 128)  0           conv2d_10[0][0]                  \n                                                                 conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 100, 100, 128 0           add[0][0]                        \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 100, 100, 64) 73792       up_sampling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 100, 100, 64) 0           conv2d_8[0][0]                   \n                                                                 conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 100, 100, 3)  1731        add_1[0][0]                      \n==================================================================================================\nTotal params: 1,110,403\nTrainable params: 1,110,403\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"ZK6yESWm_-Am","cellView":"both","executionInfo":{"status":"ok","timestamp":1602200776490,"user_tz":300,"elapsed":508,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["#@title\n","# from each of the generators create a pair of interleaved datasets\n","# tensorflow can automatically multiprocess interleaved datasets\n","# so that while batches can be loaded and processed ahead of time\n","interleaved_train = tf.data.Dataset.range(2).interleave(\n","                    get_train_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )\n","interleaved_val = tf.data.Dataset.range(2).interleave(\n","                    get_val_dataset,\n","                    num_parallel_calls=tf.data.experimental.AUTOTUNE\n","                )"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKLhe3NEAs8q","executionInfo":{"status":"error","timestamp":1602200780661,"user_tz":300,"elapsed":3226,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}},"outputId":"3bb21567-5f60-448c-a722-e8c4de117097","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True),\n","tf.keras.callbacks.ModelCheckpoint(\"data/model_checkpoints/weights.{epoch:02d}-{val_accuracy:.2f}.hdf5\", monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=True)]\n","\n","autoencoder = getNewModel()\n","\n","a_e = autoencoder.fit(interleaved_train,\n","                   steps_per_epoch = train_steps,\n","                   epochs = 5,\n","                    verbose=1,\n","                    validation_data=interleaved_val,\n","                    validation_steps=val_steps,\n","                   callbacks=callbacks)\n","\n","autoencoder.save(\"data/saved_models/autoencoder_large.h5\")"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"functional_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 100, 100, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 100, 100, 64) 1792        input_3[0][0]                    \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 50, 50, 128)  73856       max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 25, 25, 128)  0           conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 25, 25, 256)  295168      max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 50, 50, 256)  0           conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 50, 50, 128)  295040      up_sampling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 50, 50, 128)  0           conv2d_20[0][0]                  \n                                                                 conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_4 (UpSampling2D)  (None, 100, 100, 128 0           add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 100, 100, 64) 73792       up_sampling2d_4[0][0]            \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 100, 100, 64) 0           conv2d_18[0][0]                  \n                                                                 conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 100, 100, 3)  1731        add_3[0][0]                      \n==================================================================================================\nTotal params: 1,110,403\nTrainable params: 1,110,403\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'fit'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-62b151d1d66d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNewModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m a_e = autoencoder.fit(interleaved_train,\n\u001b[0m\u001b[0;32m      7\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fit'"]}]},{"cell_type":"code","metadata":{"id":"oprjongPPL7w","executionInfo":{"status":"aborted","timestamp":1602198904197,"user_tz":300,"elapsed":4473,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["from tqdm import tqdm\n","\n","def reconstruct_image(patches, ranges, weights):\n","  img = np.zeros(weights.shape)\n","  for i, (y0, y1, x0, x1) in enumerate(ranges):\n","    img[y0:y1,x0:x1] += patches[i]\n","  \n","  return np.array((img / weights) * 255, dtype=np.uint8)\n","\n","def sharpen_image(model, img):\n","  patches = []\n","  weights = np.zeros(img.shape)\n","  ranges = []\n","  for y in range(patch_size[1], img.shape[0] + 1 + (y_stride - (img.shape[0] % y_stride)), y_stride):\n","    if y > img.shape[0]:\n","      y = img.shape[0]\n","\n","    for x in range(patch_size[0], img.shape[1] + 1 + (x_stride - (img.shape[1] % x_stride)), x_stride):\n","      if x > img.shape[1]:\n","        x = img.shape[1]\n","\n","      y0 = y - patch_size[1]\n","      x0 = x - patch_size[0]\n","      patches.append(img[y0 : y, x0 : x])\n","      weights[y0 : y, x0 : x] += 1\n","      ranges.append([y0, y, x0, x])\n","\n","  patches = np.array(patches)\n","  patch_batch_count = batch_size * max_patches\n","  preds = []\n","  for i in tqdm(list(range(0, patches.shape[0], patch_batch_count))):\n","    preds.append(np.array(model.predict(patches[i : i + patch_batch_count])))\n","  \n","  preds = np.concatenate(preds)\n","\n","  return reconstruct_image(preds, ranges, weights)\n","\n","def pixalate_image(image, scale_percent = 40):\n","      width = int(image.shape[1] * scale_percent / 100)\n","      height = int(image.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","\n","      small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n","      \n","      # scale back to original size\n","      width = image.shape[1]\n","      height = image.shape[0]\n","      dim = (width, height)\n","\n","      low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n","\n","      return low_res_image\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"im0CW_slCn0D","executionInfo":{"status":"aborted","timestamp":1602198904198,"user_tz":300,"elapsed":4467,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["\"\"\"from tensorflow.keras import models \n","autoencoder = models.load_model(\"/content/drive/My Drive/frame_interpolation/test_encoder2.h5\")\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfK6npAvQ3LJ","executionInfo":{"status":"aborted","timestamp":1602198904198,"user_tz":300,"elapsed":4459,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["import cv2\n","img = keras.preprocessing.image.load_img(\"/content/drive/My Drive/frame_interpolation/test3.png\")\n","img = keras.preprocessing.image.img_to_array(img) / 255\n","dim = (img.shape[1] * 2, img.shape[0] * 2)\n","print(img.shape)\n","img = pixalate_image(img, 25)#cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n","img = np.array(img)\n","imageio.imwrite(\"/content/drive/My Drive/frame_interpolation/test3Pixelated.png\", np.array(img * 255, dtype=np.uint8))\n","print(img.shape)\n","imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRXr9wBVR6mW","executionInfo":{"status":"aborted","timestamp":1602198904199,"user_tz":300,"elapsed":4451,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["sharp_img = sharpen_image(autoencoder, img)\n","imshow(sharp_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gtzzXjzq_a9","executionInfo":{"status":"aborted","timestamp":1602198904199,"user_tz":300,"elapsed":4443,"user":{"displayName":"Phillip Merritt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifwziSGmW3vBI_Z_wA8yw0ObpGMgcwWBmqgeFRXA=s64","userId":"08126032782039777255"}}},"source":["imageio.imsave(\"/content/drive/My Drive/frame_interpolation/test3out.png\", sharp_img)"],"execution_count":null,"outputs":[]}]}